# ComfyuME Multi-User Workshop Platform Configuration
# App Version: comfyume v0.11.0 (for ComfyUI v0.11.0)
# Copy this file to .env and update with your values
# For production: Use consolidated .env from comfymulti-scripts repo
# VERSION: 0.3.2
# UPDATED: 2026-02-01

# =============================================================================
# SCRIPT METADATA
# =============================================================================
ENV_VERSION="0.3.2"
ENV_DATE="2026-02-01"

# ============================================================================
# APP CONFIGURATION
# ============================================================================

# -----------------------------------------------------------------------------
# SERVER MODE SELECTION
# -----------------------------------------------------------------------------
# Set to 'single' for single server deployment
# Set to 'dual' for dual server deployment (App Server + Inference Server)
SERVER_MODE=dual

# -----------------------------------------------------------------------------
# APP-SERVER: NGINX MODE SELECTION
# -----------------------------------------------------------------------------
# Set to 'true' to use host nginx, 'false' to use Docker nginx container
USE_HOST_NGINX=true

# -----------------------------------------------------------------------------
# APP-SERVER: NGINX CONFIGURATION
# -----------------------------------------------------------------------------
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443
# Client max body size (for uploads) note: models/video files are large!
NGINX_CLIENT_MAX_BODY_SIZE=10000M

# -----------------------------------------------------------------------------
# COMFYUI CONFIGURATION
# -----------------------------------------------------------------------------
# ComfyUI version (v0.11.0 for comfyume - pinned in Dockerfiles)
# Current: v0.11.0 (stable for comfyume rebuild)
# Latest: Check https://github.com/comfyanonymous/ComfyUI/releases
# NOTE: Do NOT use "latest" - causes deployment issues
COMFYUI_VERSION=v0.11.0

# ComfyUI deployment mode (clarifies architecture intent)
# - frontend-testing: UI only, no inference (uses --cpu flag internally)
# - worker: Full inference capability with GPU (no --cpu flag)
COMFYUI_MODE=frontend-testing

# ComfyUI listen port (internal)
COMFYUI_PORT=8188

# -----------------------------------------------------------------------------
# DEVELOPMENT / DEBUGGING
# -----------------------------------------------------------------------------
DEBUG=false
VERBOSE_LOGS=false
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# APP-SERVER:  DOMAIN & SSL CONFIGURATION
# -----------------------------------------------------------------------------
DOMAIN=workshop.example.com
# SSL certificate paths (update with your paths)
SSL_CERT_PATH=/path/to/fullchain.pem
SSL_KEY_PATH=/path/to/privkey.pem

# -----------------------------------------------------------------------------
# APP-SERVER:  QUEUE MANAGER SERVICE
# -----------------------------------------------------------------------------
QUEUE_MANAGER_HOST=queue-manager
QUEUE_MANAGER_PORT=3000
QUEUE_MANAGER_LOG_LEVEL=INFO

QUEUE_MODE=fifo                 # or round_robin, priority
ENABLE_PRIORITY=true            # Allow instructor override
JOB_TIMEOUT=3600                # 1 hour max per job (seconds)
MAX_QUEUE_DEPTH=100             # 0 = unlimited

# -----------------------------------------------------------------------------
# REDIS - Connections to queue
# -----------------------------------------------------------------------------
# For dual-server deployment:
# - APP_SERVER_REDIS_HOST: Use 'redis' (Docker network) on app server
# - INFERENCE_SERVER_REDIS_HOST: Use Tailscale IP or domain on inference workers
# For single-server deployment:
# - APP_SERVER_REDIS_HOST=redis
# - INFERENCE_SERVER_REDIS_HOST=redis

APP_SERVER_REDIS_HOST=redis
APP_SERVER_REDIS_PORT=6379
REDIS_BIND_IP=127.0.0.1
INFERENCE_SERVER_REDIS_HOST=workshop.example.com

REDIS_PASSWORD=changeme_secure_password
REDIS_PERSISTENCE=yes

# -----------------------------------------------------------------------------
# APP-SERVER: USER CONFIGURATION
# -----------------------------------------------------------------------------
NUM_USERS=20
USER_ID_PREFIX=user
USER_CREDENTIALS_FORMAT="username:password"

# -----------------------------------------------------------------------------
# COMFYUME ADMIN DASHBOARD
# -----------------------------------------------------------------------------
ADMIN_PORT=8080
ADMIN_USERNAME=admin
ADMIN_PASSWORD=changeme_admin_password

# -----------------------------------------------------------------------------
# STORAGE PATHS
# -----------------------------------------------------------------------------
# Paths are relative to docker-compose.yml location
MODELS_PATH=./data/models
OUTPUTS_PATH=./data/outputs
INPUTS_PATH=./data/inputs
WORKFLOWS_PATH=./data/workflows

# -----------------------------------------------------------------------------
# INFERENCE SERVER CONFIGURATION
# -----------------------------------------------------------------------------
# Supported providers: verda, runpod, modal, local
INFERENCE_PROVIDER=verda

# Verda Configuration (https://verda.com)
# VERDA_API_KEY=
# VERDA_INSTANCE_TYPE=h100-sxm5
# VERDA_REGION=eu-central

# -----------------------------------------------------------------------------
# INFERENCE WORKER CONFIGURATION
# -----------------------------------------------------------------------------
NUM_WORKERS=1                   # Start with 1, scale to 3 if needed
# GPU Settings
WORKER_GPU_MEMORY_LIMIT=70G     # H100 has 80GB, leave 10GB for system
WORKER_RESTART_POLICY=unless-stopped

# -----------------------------------------------------------------------------
# INFERENCE WORKER HEARTBEAT
# -----------------------------------------------------------------------------
WORKER_HEARTBEAT_TIMEOUT=60
WORKER_POLL_INTERVAL=2

# ============================================================================
# REQUIRED WORKSHOP MODELS (Download to Remote GPU)
# ============================================================================
# These models should be downloaded to data/models/shared/ on the GPU instance
#
# PRIMARY WORKSHOP MODELS (v0.11.0):
# - Flux.2 Klein (9B & 4B) - Black Forest Labs
# - LTX-2 (19B) - Lightricks
#
# NOTE: H100 80GB can hold 2-3 large models simultaneously.
# Plan model loading based on workshop schedule.

# =============================================================================
# SECRETS (DO NOT COMMIT - for reference only)
# =============================================================================
# The following variables are in the consolidated .env (comfymulti-scripts)
# but should NOT be committed to version control:
#
# VERDA_TAILSCALE_IP
# MELLO_TAILSCALE_IP
# VERDA_DEV_USER_PASSWORD
# PUB_KEY_MELLO
# PUB_KEY_VERDA
# PUB_KEY_AEON
# VERDA_SSH_PRIVATE_KEY_LOCATION
# VERDA_SSH_PRIVATE_KEY
# GH_TOKEN
# GH_APP_REPO
# GH_APP_BRANCH
# GH_SCRIPTS_REPO
# GH_SCRIPTS_BRANCH
# R2_ENDPOINT
# R2_ACCESS_KEY_ID
# R2_SECRET_ACCESS_KEY
# R2_MODELS_BUCKET
# R2_USER_FILES_BUCKET
# R2_WORKER_CONTAINER_TAR_BALL_BUCKET
# R2_CACHE_BUCKET
# UBUNTU_PRO_TOKEN
# USER_CREDENTIALS_USER001-020
