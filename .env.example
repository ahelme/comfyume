# ComfyUI Multi-User Workshop Platform Configuration
# Copy this file to .env and update with your values

# ============================================================================
# DOMAIN & SSL CONFIGURATION
# ============================================================================
DOMAIN=workshop.example.com
SSL_CERT_PATH=/path/to/fullchain.pem
SSL_KEY_PATH=/path/to/privkey.pem

# ============================================================================
# INFERENCE PROVIDER CONFIGURATION
# ============================================================================
# Supported providers: verda, runpod, modal, local
INFERENCE_PROVIDER=verda

# Verda Configuration (https://verda.com)
VERDA_API_KEY=
VERDA_INSTANCE_TYPE=h100-sxm5
VERDA_REGION=eu-central

# RunPod Configuration (https://runpod.io)
RUNPOD_API_KEY=
RUNPOD_GPU_TYPE=NVIDIA H100
RUNPOD_TEMPLATE_ID=

# Modal Configuration (https://modal.com)
MODAL_TOKEN_ID=
MODAL_TOKEN_SECRET=

# Local Configuration (for development/testing)
LOCAL_GPU_DEVICE=0

# ============================================================================
# USER CONFIGURATION
# ============================================================================
NUM_USERS=20
# User ID format: user001, user002, etc.
USER_ID_PREFIX=user

# ============================================================================
# WORKER CONFIGURATION
# ============================================================================
NUM_WORKERS=1                   # Start with 1, scale to 3 if needed
# GPU Settings
WORKER_GPU_MEMORY_LIMIT=70G     # H100 has 80GB, leave 10GB for system
WORKER_RESTART_POLICY=unless-stopped

# ============================================================================
# QUEUE CONFIGURATION
# ============================================================================
QUEUE_MODE=fifo                 # or round_robin, priority
ENABLE_PRIORITY=true            # Allow instructor override
JOB_TIMEOUT=3600                # 1 hour max per job (seconds)
MAX_QUEUE_DEPTH=100             # 0 = unlimited

# ============================================================================
# REDIS CONFIGURATION
# ============================================================================
# For split deployment: Set REDIS_HOST to VPS domain/IP on GPU workers
# VPS deployment: REDIS_HOST=redis (Docker network)
# GPU worker deployment: REDIS_HOST=comfy.ahelme.net (or VPS IP)
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=changeme_secure_password
REDIS_PERSISTENCE=yes

# ============================================================================
# QUEUE MANAGER SERVICE
# ============================================================================
QUEUE_MANAGER_HOST=queue-manager
QUEUE_MANAGER_PORT=3000
# Log level: DEBUG, INFO, WARNING, ERROR
QUEUE_MANAGER_LOG_LEVEL=INFO

# ============================================================================
# NGINX CONFIGURATION
# ============================================================================
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443
# Client max body size (for uploads)
NGINX_CLIENT_MAX_BODY_SIZE=500M

# ============================================================================
# ADMIN DASHBOARD
# ============================================================================
ADMIN_PORT=8080
# Admin authentication (optional)
ADMIN_USERNAME=admin
ADMIN_PASSWORD=changeme_admin_password

# ============================================================================
# STORAGE PATHS (Host Volumes)
# ============================================================================
# Paths are relative to docker-compose.yml location
MODELS_PATH=./data/models
OUTPUTS_PATH=./data/outputs
INPUTS_PATH=./data/inputs
WORKFLOWS_PATH=./data/workflows

# ============================================================================
# COMFYUI CONFIGURATION
# ============================================================================
# ComfyUI version (pinned for stability - see GitHub Issue #12 for updates)
# Current: v0.9.2 (pinned in Dockerfiles)
# Latest: Check https://github.com/comfyanonymous/ComfyUI/releases
# NOTE: Do NOT use "latest" - causes deployment issues
COMFYUI_VERSION=v0.9.2

# ComfyUI listen port (internal)
COMFYUI_PORT=8188

# ============================================================================
# DEVELOPMENT / DEBUGGING
# ============================================================================
# Set to 'true' for development mode
DEBUG=false
# Enable verbose logging
VERBOSE_LOGS=false

# ============================================================================
# WORKSHOP SPECIFIC
# ============================================================================
# Workshop duration (hours)
WORKSHOP_DURATION=8

# ============================================================================
# REQUIRED WORKSHOP MODELS (Download to Remote GPU)
# ============================================================================
# These models should be downloaded to data/models/shared/ on the GPU instance
#
# VIDEO MODELS:
# - LTX-2 (Lightricks - 10GB VRAM)
# - Wan 2.2 (Wand AI - 28GB VRAM)
# - HunyuanVideo-I2V (Tencent - 25GB VRAM)
# - Mochi 1 (Genmo - 18GB VRAM)
# - SkyReels V1 (SkyworkAI - 15GB VRAM)
#
# IMAGE MODELS:
# - Z-Image Turbo (Stability AI - 8GB VRAM)
# - Qwen Image-2512 (Alibaba - 12GB VRAM)
# - Qwen Image-2511 (Alibaba - 12GB VRAM)
# - Flux.2 Dev (Black Forest Labs - 24GB VRAM)
# - HunyuanImage-3.0 (Tencent - 20GB VRAM)
# - HiDream-I1 (HiDream AI - 16GB VRAM)
# - SD3.5 (Stability AI - 10GB VRAM)
# - Juggernaut XL (RunDiffusion - 8GB VRAM)
# - Stable Cascade (Stability AI - 12GB VRAM)
# - DreamShaper XL (Lykon - 8GB VRAM)
#
# 3D MODELS:
# - Ultrashape 1.0 (3D generation - 14GB VRAM)
# - HY Motion (Hunyuan Motion - 10GB VRAM)
#
# ACCELERATORS, EDITORS, UPSCALERS & MANIPULATORS:
# - Stream DiffVSR (Video super-resolution - 6GB VRAM)
# - HiStream (High-quality streaming - 8GB VRAM)
# - TwinFlow ZIT (Zero-shot image translation - 10GB VRAM)
# - ProEdit (Professional editing - 12GB VRAM)
# - SpotEdit (Spot editing - 8GB VRAM)
# - NVIDIA Nemotron Nano 2 VL (Vision-language - 4GB VRAM)
#
# WORLD MODELS:
# - Yume 1.5 (World model - 20GB VRAM)
#
# NOTE: H100 80GB can hold 2-3 large models simultaneously.
# Plan model loading based on workshop schedule.
